% http://latex.silmaril.ie/formattinginformation/writing.html

\documentclass[a4paper]{article}
\usepackage[a4paper,left=4.8em,top=4.8em,bottom=4.8em,right=4.8em]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{float}
\usepackage{subcaption}
\usepackage[bottom]{pkgs/footmisc}

\usepackage[bookmarks,pdfusetitle]{hyperref}
\hypersetup{colorlinks=true,linkcolor=.,filecolor=purple,urlcolor=teal}

\usepackage{pkgs/enumitem}
\setlist{nosep}

\setcounter{secnumdepth}{3}
\setlength{\parindent}{0em}
\setlength{\parskip}{0.4em}

\begin{document}

\title{Multilayer perceptron}
\author{alek3y}
\date{}
\maketitle

\section{Introduction}

The \textbf{multilayer perceptron} is one of the most common type of neural networks. Specifically, a feedfoward artificial neural network (\textbf{FFNN}).

The fundamental components of a neural network are:

\begin{itemize}
\item
\textbf{Weights}: They're used to determine the importance of a single neuron with respect to the next neuron.
\item
\textbf{Neurons}: They're like cells of the network that have the task of computing the \href{https://qr.ae/pGEzdE}{weighted sum} of the neurons on the previous layer (called \textit{inputs}) through the use of \textit{weights}.
\item
\textbf{Biases}: They help to determine how easy it is for the network to consider their associated neuron active\footnote{\href{https://stackoverflow.com/a/54651771}{What is the role of the bias in neural networks?}}, i.e. with a big bias it's easy for the neuron to be active, but with a negative bias it's more difficult.
\end{itemize}

All neurons of this type of perceptron are connected between each layer.

\begin{figure}[H]
\centering
\includegraphics[width=14em]{assets/ann-layers.pdf}
\caption{Neural network with one hidden layer}
\label{fig:ann-layers}
\end{figure}

\subsection{Layers}

The neurons of the network are grouped into multiple layers depending on their role.

Being the first layer, the \textbf{input layer} is the only one that doesn't alter its input, which would be the external data. \\
Consider a \textit{25 by 25 pixels} image of cat, a cute one.
For the network to recognize it you might want to feed it each pixel, which would make the input layer \textit{625 neurons} in size.

After its computations, the network processes the data on the \textbf{output layer}, which produces the ultimate result. \\
Continuing with the previous example, the output layer could consist of \textit{2 neurons}. One that classifies the image as \textit{"Dog"} and the other as \textit{"Cat"}.

Between the previously mentioned layers, there are one or more \textbf{hidden layers}.
Broadly speaking, their task is to recognize patterns by building on top of the previous ones to get more and more sophisticated features. \\
To actually classify the cat picture, the network might learn to recognize edges, then the shape of the eyes and mouth, and lastly their positions.

\subsection{Linearity}

\subsubsection{Linear perceptron}

Linear perceptrons classifiers, also called \textbf{single-layer perceptrons}, are the \textit{simplest} feedfoward neural network available, as they have no hidden layers.
They're limited to to \textit{linearly separable} datasets, which are a set of points that \textbf{can} be separated by a linear function for classification.

\begin{figure}[H]
\centering
\includegraphics[width=18em]{assets/dataset-linear.png}
\caption{Dataset separated by the linear function $f(x)$}
\label{fig:dataset-linear}
\end{figure}

As it can be seen on \autoref{fig:dataset-linear}, the straight line $f(x)$ is able to separate the set of \textit{blue} and \textit{orange} points correctly, which allows a linear perceptron to classify them.

Ideally, the network just has to learn what the values of the slope $m$ and the intercept $q$ are for the line $f(x) = mx + q$.

\subsubsection{Non-linear perceptron}

For a more complex classification, \textbf{multilayer perceptrons} are necessary.

As the name implies, for a perceptron to be \textit{multilayer} it has to have at least one hidden layer.
Furthermore, a non-linear \textit{activation function} \textbf{must} be used for the network the be non-linear itself.

% TODO: Mention forward propagation on the weighted sum
An \textbf{activation function} is a function that changes the weighted sum of a neuron to be non-linear.
Two honorable mentions, as show in \autoref{fig:activation-functions}, are:

\begin{itemize}
\item
\textbf{Sigmoid}, which is expressed as $s(x)=\frac{1}{1 + e^{-x}}$, and it maps its argument to the range $\left[0, 1\right]$.
\item
\textbf{ReLU}, expressed as $r(x)=max(0, x)$, it ignores everything but the positive part of its argument.
Although it imporoves the performance, some neurons might ultimately become inactive for all inputs, effectively dying.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=14em]{assets/activation-functions.pdf}
\caption{$r(x)$ as \textit{ReLU}, and $s(x)$ as \textit{sigmoid}}
\label{fig:activation-functions}
\end{figure}

This type of network is considered to be capable of approximating any continuous function\footnote{\href{https://www.youtube.com/watch?v=Ijqkc7OLenI}{The Universal Approximation Theorem for neural networks}}, as proven by the \href{https://en.wikipedia.org/wiki/Universal_approximation_theorem}{universal approximation theorem}.

Because of that, the neural network can learn to approximate the required non-linear function to separete a set of points in a complex dataset, like shown in \autoref{fig:dataset-nonlinear}.

\begin{figure}[H]
\centering
\includegraphics[width=18em]{assets/dataset-nonlinear.png}
\caption{Complex dataset separated by $g(x)$}
\label{fig:dataset-nonlinear}
\end{figure}

\subsection{Learning}

For the neural network to learn and improve, it has to know how good its output was.
This result is given by the \textbf{loss function}, which computes the \textbf{cost} of the entire network given their parameters (\textit{weights} and \textit{biases}).

Lowering the cost as much as possible is the goal of the network, and to do that every single parameter is going to be slightly altered.
This alteration is found with the \textit{gradient} of the loss function.

\begin{figure}[H]
\centering
\begin{subfigure}{14em}
\includegraphics[width=\textwidth]{assets/descent-local.pdf}
\caption{Local minima\footnotemark}
\end{subfigure}
\hspace{1em}
\begin{subfigure}{14em}
\includegraphics[width=\textwidth]{assets/descent-global.pdf}
\caption{Global minima}
\end{subfigure}
\caption{Gradient descent for $A_x$}
\label{fig:descent}
\end{figure}

\footnotetext{\url{https://www.desmos.com/calculator/jcksoztqbl}}

Let's take $A_x$ as one parameter to be altered on the loss function $C(x)$, as shown in both situations in \autoref{fig:descent}.

Just using the direction of the \textit{gradient} for $A_x$ would lead the point towards the fastest increase in the function, which is the opposite of that intended.
Getting the direction of fastest decrease is simply a matter of using the \textbf{negative} of said \textit{gradient}, which is referred to as \textbf{gradient descent}.

The \textit{negative gradient} would push $A$ towards $M$, moving it in $B$'s place.

\end{document}
